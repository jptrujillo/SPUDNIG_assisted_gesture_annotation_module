{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d5f8cd-0485-4d57-9902-913e6a13dfd4",
   "metadata": {},
   "source": [
    "<center><h1> Using Automated Movement Detection to Assist with Gesture Annotation </h1>\n",
    "\n",
    "\n",
    "<h4> James Trujillo ( j.p.trujillo@uva.nl )<br>\n",
    "    Updated: 19-06-2025 </h4>\n",
    "    \n",
    "<img src=\"./images/envisionbox_logo.png\" alt=\"isolated\" width=\"300\"/> \n",
    "<br>\n",
    "<h3> Info </h3>\n",
    "This module is meant to be a brief tutorial on how you can use the automatic movement detection tool SPUDNIG (Ripperda, Drijvers & Holler, 2020) as part of your gesture coding pipeline. This method can be handy because SPUDNIG can capture all of the hand movements performed by a person, allowing the coder to simply go through these annotations, determine which are gestures and which are not, and fix the timing boundaries of the annotations, rather than going through every second of the video. The authors of the original SPUDNIG paper estimate that this can cut the time you spend gesture coding nearly in half. <br>\n",
    "So, in this tutorial, we'll see how you can easily set everything up, run the program, and then use the output to code some gestures.\n",
    "<br><br>\n",
    "\n",
    "<b>Packages:</b>\n",
    "    \n",
    "* opencv-python\n",
    "\n",
    "* pympi-ling\n",
    "\n",
    "* SciPy\n",
    "\n",
    "* numpy\n",
    "\n",
    "* pandas\n",
    "\n",
    "* scipy\n",
    "\n",
    "* mediapipe\n",
    "<br><br>\n",
    " \n",
    "    \n",
    "<b>Module citation: </b>\n",
    "Trujillo, J.P. (2025). <i> Using Automated Movement Detection to Assist with Gesture Annotation </i> \\[day you visited the site]. Retrieved from: https://envisionbox.org/Assisted_Gesture_Annotation.html \n",
    "<br>\n",
    "<b>SPUDNIG reference</b>\n",
    "\n",
    "<br>Ripperda, J., Drijvers, L. & Holler, J. (2020). Speeding up the detection of non-iconic and iconic gestures (SPUDNIG): A toolkit for the automatic detection of hand movements and gestures in video data. <i>Behavior Research Methods</i>: 52, 1783â€“1794. https://doi.org/10.3758/s13428-020-01350-2 <br>\n",
    "<b>Location Repository:</b>\n",
    "\n",
    "https://github.com/jptrujillo/SPUDNIG_assisted_gesture_annotation_module\n",
    "<br><br>\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "156bcffb-3728-4583-b529-2a0ba31df5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import utils\n",
    "import movements2\n",
    "import run_MP_module\n",
    "import os\n",
    "import sort_output\n",
    "import argparse\n",
    "import pympi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db372d2-b765-4935-92ee-7aadf1e6532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(filename):\n",
    "    keypoints_left = utils.keypoint_check(keypoints_left)\n",
    "    keypoints_right = utils.keypoint_check(keypoints_right)\n",
    "    keypoints_body = utils.keypoint_check(keypoints_body)\n",
    "    \n",
    "    fps = utils.get_fps(filename)\n",
    "    return  fps, keypoints_left, keypoints_right, keypoints_body\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa0246-46d8-41f6-86ae-0275c0448a93",
   "metadata": {},
   "source": [
    "## Running SPUDNIG\n",
    "In the next step, we will actually run SPUDNIG on our videos. Most of the functions have been tucked away into imported modules to make this tutorial more concise, so there's not much to see here. However, one thing to note is the two-step process. First, we check if there is already some MediaPipe data in the <i>motion_data</i> folder. If not, we go ahead and do the motion tracking. After that, we run the SPUDNIG algorithm which does the actual movement estimation. What is important to note here is that if you already have motion tracking data, you won't have to run it again. This code block will just use what's already there (assuming it's in the motion_data folder). This also makes it a lot easier to go back and adjust parameters for movement detection, without re-running any motion tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05254ab-545a-4f31-840e-60fce88745a6",
   "metadata": {},
   "source": [
    "### Set some parameters\n",
    "Here we want to set up some parameters for how the automated detection will work, such as movement thresholds. For the first run, we'll just leave them as is. You can always adjust them and re-run the notebook if you want to fine-tune your movement detection. These values are chosen to minimize the number of false negatives and false positives, but can all be adjusted. The default values (used here) are also meant to skew somewhat towards reducing the number of false negatives as possible, at the cost of additional false positives. In other words, it ensures that you get every gesture in the video, although you will need to clean out the 'noise'.<br>\n",
    "- <b>Threshold:</b> If the reliability of a key point in a frame is below the reliability threshold, the script stops checking for potential movement and indicates that nomovement is detected in the respective frame, before continuing to the next frame. If the reliability is above the threshold, the script continues and determines whether the key point in question is part of a rest state or part of a movement. <i>range: 0 - 1</i>\n",
    "- <b>minimum cutoff:</b> minimum number of frames to be considered a movement, lower value = more precise tracking, i.e. even small movements are detected, higher value = more lenient tracking, i.e. only large movements that e xtend over several frames are detected. <i>range: 0 - 10</i>\n",
    "- <b>gap cutoff:</b> minimum number of frames between two \"movements\" before they are merged together (i.e., what constitutes a gap). lower value = more individual submovements, higher value = fewer movements, more merging. <i>range: 0 - 10</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374645e2-ac29-472d-a1c8-a002127f19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum number of frames to be considered a \"movement\" (values: 0-10)\n",
    "min_cutoff = 4\n",
    "# minimum number of frames between two \"movements\" before they are merged together (values: 0-10)\n",
    "gap_cutoff = 4\n",
    "# threshold for reliability: lower values correspond to lower reliability/precision of tracking. \n",
    "# Frames with lower reliability than this are discarded (values: 0-1)\n",
    "threshold = 0.3\n",
    "# these numbers just correspond to keypoints in MediaPipe\n",
    "keypoints_left = range(4,83)\n",
    "keypoints_right = range(4,83)\n",
    "keypoints_body = range(9,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea8d530-14e8-4898-aff4-10cde7d647f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"./videos_to_process/\"\n",
    "motion_output_folder = \"./motion_data/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad302570-574a-4104-acaf-67c4bf796305",
   "metadata": {},
   "source": [
    "This next code block will:\n",
    "- loop through each video in your <i>videos_to_process</i> folder\n",
    "- check if there is no tracking data already, and if not, run the motion tracking script\n",
    "- get the fps of the video, which is used when determining what is considered a \"movement\n",
    "- run SPUDNIG, resulting in a dataframe containing the detected movements\n",
    "- save an Elan file with these annotations, into the <i>annotations</i> folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06779001-d9c3-4544-b1f7-4c6bd8789eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Tier      Begin        End Annotation\n",
      "0  Movements  0:0:0.033  0:0:1.401   movement\n",
      "1  Movements  0:0:1.634  0:0:3.136   movement\n",
      "2  Movements  0:0:3.603  0:0:5.872   movement\n",
      "         Tier       Begin         End Annotation\n",
      "0   Movements   0:0:0.200   0:0:5.100   movement\n",
      "1   Movements   0:0:5.466   0:0:8.533   movement\n",
      "2   Movements   0:0:8.833   0:0:9.100   movement\n",
      "3   Movements   0:0:9.466  0:0:10.800   movement\n",
      "4   Movements  0:0:11.200  0:0:17.100   movement\n",
      "5   Movements  0:0:17.433  0:0:21.300   movement\n",
      "6   Movements  0:0:22.066  0:0:22.833   movement\n",
      "7   Movements  0:0:23.100  0:0:28.033   movement\n",
      "8   Movements  0:0:28.266  0:0:28.833   movement\n",
      "9   Movements  0:0:29.066  0:0:29.633   movement\n",
      "10  Movements  0:0:30.766  0:0:32.100   movement\n",
      "11  Movements  0:0:32.300  0:0:37.466   movement\n",
      "12  Movements  0:0:39.266  0:0:39.800   movement\n",
      "13  Movements  0:0:40.166  0:0:50.300   movement\n",
      "14  Movements  0:0:51.000  0:0:51.466   movement\n",
      "15  Movements  0:0:52.766  0:0:54.866   movement\n",
      "16  Movements  0:0:55.100  0:0:55.266   movement\n",
      "17  Movements  0:0:55.900  0:0:58.300   movement\n",
      "18  Movements  0:0:58.500   0:1:1.066   movement\n",
      "19  Movements   0:1:1.333   0:1:3.100   movement\n",
      "20  Movements   0:1:3.633   0:1:5.233   movement\n",
      "21  Movements   0:1:5.466   0:1:6.100   movement\n",
      "22  Movements   0:1:6.600   0:1:9.266   movement\n",
      "23  Movements   0:1:9.600  0:1:10.700   movement\n",
      "24  Movements  0:1:11.133  0:1:17.033   movement\n",
      "25  Movements  0:1:17.333  0:1:18.466   movement\n",
      "26  Movements  0:1:18.733  0:1:19.566   movement\n",
      "27  Movements  0:1:19.766  0:1:31.666   movement\n",
      "28  Movements  0:1:31.966  0:1:45.733   movement\n",
      "29  Movements  0:1:46.133  0:1:46.266   movement\n",
      "30  Movements  0:1:47.200  0:1:50.666   movement\n",
      "31  Movements  0:1:50.900  0:1:51.200   movement\n",
      "32  Movements  0:1:51.533  0:1:54.166   movement\n",
      "33  Movements  0:1:55.100  0:1:59.900   movement\n",
      "34  Movements   0:2:0.866   0:2:3.500   movement\n",
      "35  Movements   0:2:3.833  0:2:15.366   movement\n",
      "36  Movements  0:2:19.966  0:2:21.266   movement\n",
      "37  Movements  0:2:21.666  0:2:22.033   movement\n",
      "38  Movements  0:2:25.466  0:2:25.700   movement\n",
      "39  Movements  0:2:26.133  0:2:28.533   movement\n"
     ]
    }
   ],
   "source": [
    "for video_name in os.listdir(video_dir):\n",
    "\n",
    "    video_name_short =  video_name.split(\".\")[0]\n",
    "    data_output_folder = motion_output_folder + \"/\" + video_name_short + \"/\" \n",
    "    \n",
    "    if not os.path.isfile(data_output_folder + video_name):\n",
    "            run_MP_module.process_video(video_name, video_dir, data_output_folder)\n",
    "        \n",
    "    #### Now run SPUDNIG ####\n",
    "    # first, restructure the data for SPUDNIG\n",
    "    keypoints_left, keypoints_right, keypoints_body = sort_output.sort_MP(\n",
    "       data_output_folder, keypoints_left, keypoints_right, keypoints_body, video_name_short)\n",
    "    # then, get the video's fps\n",
    "    fps = utils.get_fps(video_dir + video_name)\n",
    "\n",
    "    # now process\n",
    "    data = movements2.main(data_output_folder, threshold, keypoints_left, keypoints_right, keypoints_body, fps,\n",
    "                           min_cutoff, gap_cutoff)\n",
    "    \n",
    "    print(data)\n",
    "\n",
    "    Annotation = [(timestamp_to_ms(row[1][\"Begin\"]), timestamp_to_ms(row[1][\"End\"]),row[1][\"Annotation\"]) for row in data.iterrows()]\n",
    "    tiers = {'Movements':Annotation}\n",
    "    new_eaf = utils.to_eaf(tiers)\n",
    "                    \n",
    "    new_eaf.to_file(\"./annotations/\" + video_name_short + \".eaf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2df945-9911-4c4d-9ec8-4db83a49f643",
   "metadata": {},
   "source": [
    "## Checking the motion tracking\n",
    "This first step is basically just a sanity check to make sure that the tracking data makes sense. All this entails, is that you open the video file in the motion_data folder and check if the keypoints make sense. Is it (relatively consistently) putting keypointso on the shoulders, hands, etc? It won't be perfect, but you want to make sure there's not anything strange going on in the video. <br>\n",
    "Ideally, you have something like this:<br>\n",
    "<img src=\"./images/good_tracking.png\" width=500 />\n",
    "\n",
    "Sometimes there will be tracking errors. Take a look at the example below: <br>\n",
    "<img src=\"./images/poor_tracking.png\" width=500 />\n",
    "<br>\n",
    "Here we see some points where tracking did not work well. However, most of the video looks okay. This is really a judgment call on whether you trust the quality of the tracking, and there's not really a standard on this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b720ffa-f1ca-4fcb-aa25-248dba8cec48",
   "metadata": {},
   "source": [
    "## Cleaning the annotations\n",
    "Finally, in this step we're going to get to some usable annotations. Go ahead and open the Elan file, that will be the <i>annotations</i> folder. You can then go to Edit >> Linked Files to add the video back in: <br>\n",
    "<img src=\"./images/linked_files1.png\" width=500 /> <br>\n",
    "<br>\n",
    "\n",
    "Now, as noted before, we can do our \"assisted annotation\". We'll create a new tier called \"gesture\". <br>\n",
    "<img src=\"./images/add_tier.png\" width=500 /> <br>\n",
    "<br>\n",
    "\n",
    "Now, we can move through each of the annotations in the \"movements\" tier and see if it corresponds to a true gesture or not. If so, we can select the annotation and use this to create a new annotation in the gesture tier. When the \"movement\" annotation isn't a gesture (or perhaps just not the kind that we are interested in), we just ignore it and move on. <br>\n",
    "<img src=\"./images/add_gesture_annot.png\" width=500 /> <br>\n",
    "<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
